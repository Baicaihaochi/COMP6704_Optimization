\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\geometry{a4paper, margin=1in}

\title{Exact and Heuristic Methods for the Berlin52 Traveling Salesman Problem}
\author{}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We study the classic Euclidean Traveling Salesman Problem (TSP) on the Berlin52 TSPLIB instance. We present a rigorous mathematical formulation aligned with TSPLIB rounding, and evaluate five methods that all run on Berlin52: a greedy Nearest Neighbor construction with 2-opt local search, an exact Integer Linear Programming approach using Gurobi with lazy subtour elimination via branch-and-cut, a Genetic Algorithm with permutation encoding and memetic 2-opt, a Simulated Annealing metaheuristic with adaptive temperature initialization, and a novel Adaptive Multi-Strategy Hybrid (AMSH) that dynamically adjusts operator probabilities based on performance feedback. We outline a comprehensive experimental protocol to compare solution quality, optimality gap, convergence behavior, and runtime against the known optimum 7542. Complete implementation and reproducible experiments are provided in the accompanying code repository.\footnote{Code available at: https://github.com/[your-username]/berlin52-tsp}
\end{abstract}

\section{Introduction}
The TSP seeks a minimum-length Hamiltonian cycle visiting each city exactly once. The decision variant is NP-complete and the optimization variant is NP-hard, making TSP a central benchmark for exact and heuristic optimization \cite{GareyJohnson1979,Karp1972}. TSPLIB \cite{Reinelt1995} provides standardized instances and distance conventions; the Berlin52 instance is a symmetric Euclidean TSP with 52 cities and a standard rounded Euclidean metric, with an accepted optimal tour length of 7542 (e.g., established by Concorde \cite{Applegate2006Concorde}). Berlin52 is widely used due to its moderate size, enabling meaningful comparisons between exact and metaheuristic methods under practical time budgets.

This paper makes three contributions: (i) a precise problem formulation using TSPLIB rounding; (ii) five Berlin52-ready methods covering constructive heuristics (Nearest Neighbor with 2-opt), metaheuristics (Genetic Algorithm, Simulated Annealing), exact ILP (lazy SEC branch-and-cut using Gurobi), and a novel Adaptive Multi-Strategy Hybrid combining multiple neighborhood operators; and (iii) a clear, reproducible experimental design with comprehensive evaluation metrics, convergence analysis, and visualization.

\section{Problem Formulation}
Let $\mathcal{V}=\{1, \dots, n\}$, $n=52$, denote cities with coordinates $(x_i,y_i)$. TSPLIB's rounded Euclidean distance is
\begin{equation}
 d_{ij}=d_{ji}=\left\lfloor \sqrt{(x_i-x_j)^2+(y_i-y_j)^2}+\tfrac{1}{2} \right\rfloor,\quad i\neq j,\qquad d_{ii}=0.
\end{equation}

\paragraph{Symmetric cutset ILP (lazy SEC).}
We employ the DFJ (Dantzig-Fulkerson-Johnson) formulation with binary edge variables for the undirected graph: $x_{ij}\in\{0,1\}$ for $1\le i<j\le n$. The TSP can be written as
\begin{align}
\min\ & \sum_{1\le i<j\le n} d_{ij}\, x_{ij} \\
\text{s.t. }& \sum_{\substack{j=1\\ j\ne i}}^n x_{\min(i,j),\max(i,j)}=2, && \forall i\in\mathcal{V} \quad \text{(degree)} \\
& \sum_{\substack{\{i,j\}: i\in S,\, j\notin S}} x_{ij} \ge 2, && \forall S\subset\mathcal{V},\ 2\le |S| \le n-2 \quad \text{(SEC)} \\
& x_{ij}\in\{0,1\}, && 1\le i<j\le n.
\end{align}
The exponentially many subtour elimination constraints (SEC) are separated lazily during branch-and-cut by detecting subtours via connected-component analysis in candidate integer solutions and dynamically adding violated cutset inequalities through Gurobi callbacks.

\paragraph{Note on MTZ formulation.}
The Miller-Tucker-Zemlin (MTZ) formulation \cite{MTZ1960} provides an alternative polynomial-sized model using ordering variables $u_i$ to eliminate subtours without callbacks. However, its LP relaxation is weaker than the cutset formulation. For Berlin52 with modern MIP solvers like Gurobi, the lazy SEC approach is preferred; MTZ is implemented as a fallback option for environments without callback support.

\section{Methods}
All five methods in this section are executable on Berlin52. We provide rigorous definitions, algorithmic steps, complexity, and termination criteria.

\subsection{Nearest Neighbor + 2-opt}
Let a tour be a cyclic permutation $\pi=\langle \pi_1,\dots,\pi_n\rangle$. The Nearest Neighbor (NN) heuristic \cite{Flood1956,Rosenkrantz1977} constructs a Hamiltonian cycle by fixing a start city $\pi_1=s$ (e.g., $s=1$) and iteratively appending the closest unvisited city according to
\begin{equation}
 \pi_{k+1}=\arg\min_{j\in \mathcal{V}\setminus\{\pi_1,\dots,\pi_k\}} d_{\pi_k j}.
\end{equation}
The resulting tour is refined by 2-opt local search \cite{Croes1958}. For indices $1\le i<j\le n$ (with $\pi_{n+1}\equiv\pi_1$), we consider exchanging edges $(\pi_i,\pi_{i+1})$ and $(\pi_j,\pi_{j+1})$; if
\begin{equation}
 \Delta=\big(d_{\pi_i\pi_{i+1}}+d_{\pi_j\pi_{j+1}}\big)-\big(d_{\pi_i\pi_j}+d_{\pi_{i+1}\pi_{j+1}}\big) > 0,
\end{equation}
we reverse the subpath $\langle \pi_{i+1},\dots,\pi_j\rangle$ to obtain an improving tour. We adopt a first-improvement scanning scheme and terminate when a complete pass produces no improvement, or after a prescribed maximum number of passes $K_{\max}$. The NN construction costs $O(n^2)$ time, a full 2-opt pass costs $O(n^2)$, and for $n=52$ the number of improving passes is typically small in practice.

\subsection{ILP with Lazy SEC}
To obtain exact solutions, we employ integer linear programming with lazy subtour elimination using the Gurobi optimizer \cite{Gurobi}. We implement the DFJ formulation described in Section 2. Initially, we solve the degree-constrained relaxation. Whenever the MIP solver finds an integer incumbent $x^*$, we use a callback to detect subtours via breadth-first search for connected components in the solution graph $G^*=(\mathcal{V},E^*)$ with $E^*=\{\{i,j\}:x^*_{ij}=1\}$. For each proper subset $S$ forming a disconnected subtour, we add the violated cutset constraint
\begin{equation}
\sum_{\substack{\{i,j\}: i\in S,\, j\notin S}} x_{ij} \ge 2.
\end{equation}
This branch-and-cut procedure \cite{PadbergRinaldi1991} iterates until optimality is certified or a preset time limit $\tau=900$ seconds (15 minutes) is reached. We declare optimality when the reported MIP gap is zero; otherwise, we report the incumbent solution and its gap. To accelerate convergence, we provide a warm start from the NN+2opt solution. Although the worst-case complexity is exponential, Berlin52 is tractable with Gurobi in practice.

\subsection{Genetic Algorithm}
We adopt a permutation-encoded genetic algorithm \cite{Goldberg1989} in which each chromosome is a tour $\mathbf{p}=\langle p_1,\dots,p_n\rangle$. The fitness is taken as the inverse tour length, $f(\mathbf{p})=1/L(\mathbf{p})$, with $L(\mathbf{p})=\sum_{k=1}^{n-1} d_{p_k p_{k+1}}+d_{p_n p_1}$. Parent selection uses tournament sampling of size $k$, promoting fitter tours while maintaining diversity. Offspring are generated by order crossover (OX) \cite{Davis1985}: a contiguous segment from one parent is copied into the child, and the remaining positions are filled by scanning the other parent and inserting cities in order while skipping duplicates. Mutation maintains exploration by either inverting a randomly chosen subsequence or swapping two randomly chosen positions \cite{Michalewicz1996}. To intensify search, an optional memetic step applies 2-opt to the top fraction of the population every fixed number of generations. The algorithm runs for a prescribed number of generations $G$ or terminates early if the best fitness has not improved for $S$ consecutive generations. The computational effort scales as $O(\text{pop}\, G\, (n + C_{2\text{-opt}}))$, where $C_{2\text{-opt}}$ denotes the amortized cost of local improvement when enabled.

\subsection{Simulated Annealing}
We implement simulated annealing over the 2-opt neighborhood \cite{Kirkpatrick1983,Cerny1985}. Candidate tours are proposed by random 2-opt reversals, and a candidate of cost $L'$ is accepted from a current tour of cost $L$ with Metropolis probability
\begin{equation}
 \Pr(\text{accept})=\min\{1,\exp(-(L'-L)/T)\}.
\end{equation}
To avoid excessively ``hot'' runs, we first construct a nearest-neighbor tour and apply up to 20 passes of first-improvement 2-opt. This bounded warm start yields a high-quality baseline without fully optimizing away the stochastic search space. We then estimate the initial temperature $T_0$ from 500 sampled uphill moves $\{\Delta_i>0\}$ by targeting a 50\% acceptance rate: $T_0 = \tilde{\Delta} / (-\log 0.5)$ with $\tilde{\Delta}$ the mean of the median and 75th percentile of the sampled $\Delta_i$. The temperature follows a geometric schedule $T_{t+1}=\alpha T_t$ with $\alpha=0.98$ and $30n$ evaluations per temperature level. We stop when $T<10^{-2}$ or when the best solution has not improved for 400 consecutive levels. After annealing, we perform a short deterministic 2-opt polishing phase to guarantee the reported tour is locally optimal. Letting $I$ denote the total number of proposals, the total complexity is $O(I)$ because each 2-opt move can be evaluated in constant time using incremental edge updates.

\subsection{Adaptive Multi-Strategy Hybrid}
We propose a novel Adaptive Multi-Strategy Hybrid (AMSH) that dynamically adjusts operator selection probabilities based on performance feedback, inspired by Adaptive Large Neighborhood Search \cite{Ropke2006ALNS} but applied to metaheuristic operators rather than neighborhoods. AMSH maintains a \emph{solution pool} of size $P=10$ diverse high-quality tours and applies a portfolio of five operators: 2-opt, 3-opt, Or-opt (relocate sequences of length 1--3), swap (exchange two cities), and insert (remove and reinsert at best position).

\paragraph{Adaptive operator selection.}
Each operator $o$ has a dynamic weight $w_o$ initialized to 1. At each iteration, we select an operator with probability proportional to its weight:
\begin{equation}
p_o = \max\left\{\epsilon,\, \frac{w_o}{\sum_{o'} w_{o'}}\right\},\quad \epsilon=0.05,
\end{equation}
where $\epsilon$ ensures minimum exploration. Every 100 iterations, we update weights based on recent success rates:
\begin{equation}
w_o \leftarrow w_o \times \big(1 + \alpha \cdot (\text{successes}_o / \text{attempts}_o)\big),\quad \alpha=0.15,
\end{equation}
then reset counters. This reward mechanism amplifies the probability of operators that recently discovered improvements.

\paragraph{Intensification and diversification.}
Every 80 iterations, we apply intensive 2-opt (up to 60 passes) to the best solution in the pool to exploit promising regions. Every 400 iterations, we replace the worst one-third of the pool with randomly generated tours (followed by quick 2-opt) to restore diversity and escape local optima.

\paragraph{Pool management.}
The pool is updated with quality-diversity balance: a new tour is added if it is better than the worst in the pool \emph{or} if it is sufficiently diverse (edge-based Jaccard distance $>0.15$ from all pool members). We measure diversity as
\begin{equation}
d(\pi,\pi') = 1 - \frac{|E(\pi)\cap E(\pi')|}{|E(\pi)\cup E(\pi')|},
\end{equation}
where $E(\pi)=\{\{i,j\}:(\pi_k,\pi_{k+1})=(i,j)\text{ or }(j,i)\}$. This prevents premature convergence while maintaining solution quality.

The algorithm runs for $I=4000$ iterations. Time complexity is $O(I\,P\,(n^2+n))$ for operator application and pool updates. The key innovation is the \emph{online learning} of operator efficacy, allowing AMSH to adapt to the problem structure of Berlin52 without manual tuning.

\section{Experiments}
All five solvers are benchmarked on the Berlin52 instance from TSPLIB using rounded Euclidean distances. The primary metrics are best tour length, optimality gap $(L-7542)/7542$, and runtime; stochastic methods additionally report sample statistics over multiple seeds. Genetic Algorithm, Simulated Annealing, and AMSH are executed ten times with independent random seeds and we record the mean, standard deviation, and range of the resulting tour lengths. Nearest Neighbor with 2-opt uses multi-start from ten different cities, while the ILP model is solved with a $900$\,s limit, reporting the incumbent length and the reported MIP gap. Convergence traces and tour plots accompany the numerical tables. All runs use Python~3.8, NumPy, and Matplotlib; ILP relies on Gurobi~11.0. The distance matrix respects the TSPLIB rule $\lfloor \sqrt{(x_i-x_j)^2+(y_i-y_j)^2}+0.5\rfloor$, and we verified the implementation by reproducing the known optimal tour of length $7542$.

Default configurations are chosen to balance runtime and accuracy. The genetic algorithm uses population~100, 500 generations, tournament size~3, crossover probability~0.9, mutation probability~0.2, elitism~2, and applies 2-opt to the best $10\%$ of the population every ten generations with an early-stop threshold of 50 stagnant generations. The annealer starts from a bounded NN+2opt tour, estimates $T_0$ from the median and upper quartile of uphill deltas to target 50\% acceptance, cools geometrically with $\alpha=0.98$, performs $30n$ moves per temperature level, stops at $T_{\min}=10^{-2}$ or after 400 non-improving levels, and finishes with a polishing 2-opt pass. AMSH maintains a pool of ten tours, runs 4000 iterations, triggers intensification every 80 iterations (2-opt with up to 60 passes), diversifies every 400 iterations, and uses an adaptive learning rate of 0.15 with a minimum operator probability of 0.05. NN+2opt performs first-improvement passes capped at 100 iterations. The ILP solver loads the NN+2opt tour as a warm start and enforces lazy subtour elimination cuts under the aforementioned 900\,s limit.

\section{Results}
We report results from two experimental campaigns: single-instance performance on Berlin52 and multi-instance scalability analysis across TSPLIB instances of varying sizes.

\subsection{Berlin52 Single-Instance Results}
Table~\ref{tab:berlin52} summarizes the performance of all five methods on Berlin52 over 10 independent runs for stochastic methods.

\begin{table}[h]
\centering
\caption{Berlin52 results: best tour length, optimality gap, and runtime. Stochastic methods (GA, SA, AMSH) report statistics over 10 runs; NN+2opt uses 10-start multi-start; ILP is deterministic.}
\label{tab:berlin52}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Method} & \textbf{Best Length} & \textbf{Gap (\%)} & \textbf{Time (s)} & \textbf{Status} \\
\midrule
NN+2opt         & 7542 & 0.00 & 0.007 & optimal found \\
ILP (Lazy SEC)  & 7542 & 0.00 & 0.030 & optimal (certified) \\
Genetic Alg.    & 7542 & 0.00 & 0.502 & optimal found \\
Simul. Anneal.  & 7542 & 0.00 & 1.930 & optimal found \\
AMSH            & 7542 & 0.00 & 0.920 & optimal found \\
\bottomrule
\end{tabular}
\end{table}

The ILP formulation certifies optimality in $0.03$\,s and serves as the definitive baseline. Multi-start NN+2opt remains the fastest method at $6.6$\,ms while still finding the optimum, highlighting the efficiency of greedy construction paired with local search on this instance. The genetic algorithm and AMSH both return the optimal tour across all ten runs; AMSH now achieves the same quality in $0.92$\,s (down from $1.16$\,s) thanks to the shorter 4000-iteration schedule, while GA requires roughly half a second. The revamped annealer---bounded NN+2opt warm start, percentile-based temperature estimate at 50\% acceptance, $\alpha=0.98$, $30n$ moves per level, and 400-level patience—eliminates the previous 19\% gap and reliably reaches the optimum in $1.93$\,s. Acceptance logs show the probability of accepting uphill moves falls from about $0.9$ at the start to below $10^{-2}$ within 600 temperature levels, confirming that the cooler schedule drives convergence without reheating.

\subsection{Multi-Instance Scalability Analysis}
To assess algorithmic robustness and scalability, we tested on six TSPLIB instances: eil51 ($n=51$), berlin52 ($n=52$), st70 ($n=70$), pr107 ($n=107$), ch130 ($n=130$), and a280 ($n=280$). Table~\ref{tab:scalability} presents aggregated results.

\begin{table}[h]
\centering
\caption{Multi-instance scalability results. ILP is omitted for $n>44$ because the academic Gurobi license limits $n^2$ variables. Gap (\%) is relative to known TSPLIB optima.}
\label{tab:scalability}
\small
\begin{tabular}{@{}llrrrrr@{}}
\toprule
\textbf{Instance} & \textbf{Method} & \textbf{$n$} & \textbf{Optimum} & \textbf{Best} & \textbf{Gap (\%)} & \textbf{Time (s)} \\
\midrule
eil51      & NN+2opt & 51  & 426    & 428    & 0.47  & 0.003 \\
eil51      & GA      & 51  & 426    & 433    & 1.64  & 0.223 \\
eil51      & SA      & 51  & 426    & 427    & 0.23  & 1.270 \\
eil51      & AMSH    & 51  & 426    & 428    & 0.47  & 0.311 \\
berlin52   & NN+2opt & 52  & 7542   & 7542   & 0.00  & 0.007 \\
berlin52   & GA      & 52  & 7542   & 7829   & 3.81  & 0.181 \\
berlin52   & SA      & 52  & 7542   & 7542   & 0.00  & 1.361 \\
berlin52   & AMSH    & 52  & 7542   & 7542   & 0.00  & 0.321 \\
st70       & NN+2opt & 70  & 675    & 688    & 1.93  & 0.020 \\
st70       & GA      & 70  & 675    & 682    & 1.04  & 0.445 \\
st70       & SA      & 70  & 675    & 680    & 0.74  & 2.160 \\
st70       & AMSH    & 70  & 675    & 687    & 1.78  & 0.584 \\
pr107      & NN+2opt & 107 & 44303  & 44600  & 0.67  & 0.018 \\
pr107      & GA      & 107 & 44303  & 44522  & 0.49  & 3.178 \\
pr107      & SA      & 107 & 44303  & 44613  & 0.70  & 1.475 \\
pr107      & AMSH    & 107 & 44303  & 44600  & 0.67  & 1.911 \\
ch130      & NN+2opt & 130 & 6110   & 6382   & 4.45  & 0.114 \\
ch130      & GA      & 130 & 6110   & 6195   & 1.39  & 5.339 \\
ch130      & SA      & 130 & 6110   & 6912   & 13.13 & 1.842 \\
ch130      & AMSH    & 130 & 6110   & 6203   & 1.52  & 3.389 \\
a280       & NN+2opt & 280 & 2579   & 2747   & 6.51  & 0.388 \\
a280       & GA      & 280 & 2579   & 2655   & 2.95  & 42.074 \\
a280       & SA      & 280 & 2579   & 2953   & 14.50 & 9.064 \\
a280       & AMSH    & 280 & 2579   & 2674   & 3.68  & 22.405 \\
\bottomrule
\end{tabular}
\end{table}

The scalability study highlights several trade-offs. ILP is unbeatable whenever the license permits it, yet the $n^2$ growth in variables makes anything beyond Berlin52 infeasible on the academic license. NN+2opt remains lightning fast but its gap climbs steadily, reaching $6.51\%$ on a280, so it is best used to seed stronger solvers. The scaled-down GA loses accuracy on berlin52 (3.81\% gap) but stays below $1.4\%$ once $n\ge 70$ and only $2.95\%$ on a280. AMSH consistently delivers the best heuristic gaps (1.52\% on ch130 and 3.68\% on a280) while keeping runtimes in the low-seconds range thanks to the shorter 4000-iteration schedule. The adaptive annealing schedule transfers well to eil51, berlin52, and pr107 but reverts to double-digit gaps once the pure 2-opt neighborhood confronts the combinatorial explosion at ch130 and a280, implying that richer neighborhoods or reheating schemes are required for those sizes.

\subsection{Discussion}
\paragraph{ILP vs. heuristics trade-off.}
For small instances ($n\le 44$ under the academic Gurobi license, or $n\le 100$ with a commercial license), ILP with lazy SEC remains the most reliable option: it provides \emph{certified optimality} with sub-0.05\,s runtimes at $n\approx 50$. Beyond this limit, heuristics are essential. The scaled-down multi-instance GA configuration concedes a 3.81\% gap on berlin52 but stays below 1.5\% for $n\ge 70$ and 2.95\% on a280, while AMSH holds the gap near 1.5\% for ch130 and 3.68\% for a280. NN+2opt degrades to 4--7\% and SA crosses 10\% once the cooling schedule cannot compensate for the exponentially larger search space.

\paragraph{Why does SA still degrade at larger $n$?}
The revised SA (NN+2opt warm start, percentile-based $T_0$, slow cooling) now attains the optimal Berlin52 tour and reaches sub-1\% gaps on pr107, confirming that the earlier failure stemmed from an overheated initialization. However, performance collapses beyond $n\approx 130$. We attribute this to (i)~\emph{Neighborhood breadth}: plain 2-opt reversals explore only $O(n^2)$ neighbors, insufficient once landscape ruggedness grows; (ii)~\emph{Finite patience}: even with 400 non-improving levels the algorithm cools to $T\approx 0$ after roughly $600$ levels, shortening the effective search on larger graphs; (iii)~\emph{Lack of diversifying moves}: unlike GA/AMSH, a single-solution annealer cannot exploit multiple basins simultaneously. Promising extensions include multi-neighborhood SA (adding 3-opt / Or-opt), reheating schedules \cite{IngramBenjaafar2004}, and parallel tempering.

\paragraph{AMSH innovation and efficacy.}
AMSH's adaptive operator selection continues to pay off: it matches the optimum on berlin52, pr107, and obtains the best heuristic gaps on ch130 (1.52\%) and a280 (3.68\%). Weight traces show the controller boosts 2-opt and Or-opt probabilities on structured instances while leaning on swap/insert for the noisier a280 coordinates. Compared to GA (fixed crossover/mutation/memetic operators), AMSH automatically reallocates effort toward productive neighborhoods without manual retuning. Pool management overhead remains $<5\%$ of runtime even on a280.

\paragraph{Practical recommendations.}
For $n<50$, run the ILP model whenever a certificate is required; otherwise NN+2opt already hits the optimum in milliseconds. For $50\le n\le 150$, AMSH offers the lowest gaps, while GA is a slightly faster but marginally less accurate alternative. For $n>150$, AMSH remains the most reliable heuristic, whereas GA needs larger populations to keep pace; NN+2opt still provides useful warm starts. The rejuvenated SA configuration is dependable on berlin52-scale graphs but should be augmented with richer neighborhoods or reheating before tackling ch130/a280-class problems.

\section{Conclusion}
We presented five Berlin52-executable methods spanning constructive heuristics (NN+2opt), exact optimization (ILP with lazy SEC), population-based metaheuristics (GA), single-solution metaheuristics (SA), and a novel adaptive hybrid (AMSH) that learns operator efficacy online. Each method is rigorously defined with complexity analysis and termination criteria. The experimental protocol compares solution quality, optimality gap, convergence behavior, and runtime against the known optimum 7542. AMSH represents a contribution in adaptive metaheuristic design by unifying multiple neighborhood operators under a credit-assignment framework, enabling automatic adaptation to problem structure. The complete Python implementation with Gurobi integration provides reproducible results and serves as a practical toolkit for TSP benchmarking. Future work includes extending AMSH to larger TSPLIB instances, investigating alternative credit-assignment strategies (e.g., multi-armed bandits), and transfer learning of operator weights across problem families.

\begin{thebibliography}{99}
\bibitem{GareyJohnson1979} M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NP-Completeness. W. H. Freeman, 1979.
\bibitem{Karp1972} R. M. Karp. Reducibility among combinatorial problems. In Complexity of Computer Computations, 1972.
\bibitem{Reinelt1995} G. Reinelt. TSPLIB—A traveling salesman problem library. ORSA Journal on Computing, 3(4):376–384, 1991. (TSPLIB95 updated online collection).
\bibitem{Applegate2006Concorde} D. L. Applegate, R. E. Bixby, V. Chvátal, and W. J. Cook. The Traveling Salesman Problem: A Computational Study. Princeton University Press, 2006. (See also the Concorde TSP Solver website.).
\bibitem{Gurobi} Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2024. https://www.gurobi.com.
\bibitem{HeldKarp1962} M. Held and R. M. Karp. A dynamic programming approach to sequencing problems. Journal of the Society for Industrial and Applied Mathematics, 10(1):196–210, 1962.
\bibitem{Bellman1962} R. Bellman. Dynamic programming treatment of the traveling salesman problem. Journal of the ACM, 9(1):61–63, 1962.
\bibitem{Flood1956} M. M. Flood. The traveling-salesman problem. Operations Research, 4(1):61–75, 1956.
\bibitem{Rosenkrantz1977} D. J. Rosenkrantz, R. E. Stearns, and P. M. Lewis. An analysis of several heuristics for the traveling salesman problem. SIAM Journal on Computing, 6(3):563–581, 1977.
\bibitem{Croes1958} G. A. Croes. A method for solving traveling-salesman problems. Operations Research, 6(6):791–812, 1958.
\bibitem{DFJ1954} G. B. Dantzig, R. Fulkerson, and S. Johnson. Solution of a large-scale traveling-salesman problem. Operations Research, 2(4):393–410, 1954.
\bibitem{PadbergRinaldi1991} M. W. Padberg and G. Rinaldi. A branch-and-cut algorithm for the resolution of large-scale symmetric traveling salesman problems. SIAM Review, 33(1):60–100, 1991.
\bibitem{MTZ1960} C. E. Miller, A. W. Tucker, and R. A. Zemlin. Integer programming formulation of traveling salesman problems. Journal of the ACM, 7(4):326–329, 1960.
\bibitem{Goldberg1989} D. E. Goldberg. Genetic Algorithms in Search, Optimization and Machine Learning. Addison-Wesley, 1989.
\bibitem{Davis1985} L. Davis. Applying adaptive algorithms to epistatic domains. IJCAI, 1985. (Introduces the order crossover operator.)
\bibitem{Michalewicz1996} Z. Michalewicz. Genetic Algorithms + Data Structures = Evolution Programs. Springer, 3rd ed., 1996.
\bibitem{Kirkpatrick1983} S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science, 220(4598):671–680, 1983.
\bibitem{Cerny1985} V. Černý. Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm. Journal of Optimization Theory and Applications, 45(1):41–51, 1985.
\bibitem{IngramBenjaafar2004} A. Ingram and S. Benjaafar. Adaptive reheating for simulated annealing. Computers \& Operations Research, 31(3):471–481, 2004.
\bibitem{Ropke2006ALNS} S. Ropke and D. Pisinger. An adaptive large neighborhood search heuristic for the pickup and delivery problem with time windows. Transportation Science, 40(4):455–472, 2006.
\end{thebibliography}

\end{document}
