\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\geometry{a4paper, margin=1in}

\title{Exact and Heuristic Methods for the Berlin52 Traveling Salesman Problem}
\author{}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We study the classic Euclidean Traveling Salesman Problem (TSP) on the Berlin52 TSPLIB instance. We present a rigorous mathematical formulation aligned with TSPLIB rounding, and evaluate five methods that all run on Berlin52: a greedy Nearest Neighbor construction with 2-opt local search, an exact Integer Linear Programming approach using Gurobi with lazy subtour elimination via branch-and-cut, a Genetic Algorithm with permutation encoding and memetic 2-opt, a Simulated Annealing metaheuristic with adaptive temperature initialization, and a novel Adaptive Multi-Strategy Hybrid (AMSH) that dynamically adjusts operator probabilities based on performance feedback. We outline a comprehensive experimental protocol to compare solution quality, optimality gap, convergence behavior, and runtime against the known optimum 7542. Complete implementation and reproducible experiments are provided in the accompanying code repository.\footnote{Code available at: https://github.com/[your-username]/berlin52-tsp}
\end{abstract}

\section{Introduction}
The Traveling Salesman Problem (TSP) seeks a minimum-length Hamiltonian cycle visiting each city exactly once. The decision variant is NP-complete and the optimization variant is NP-hard, making TSP a central benchmark for exact and heuristic optimization \cite{GareyJohnson1979,Karp1972}. TSPLIB \cite{Reinelt1995} provides standardized instances and distance conventions; the Berlin52 instance is a symmetric Euclidean TSP with 52 cities and a standard rounded Euclidean metric, with an accepted optimal tour length of 7542 (e.g., established by Concorde \cite{Applegate2006Concorde}). Berlin52 is widely used due to its moderate size, enabling meaningful comparisons between exact and metaheuristic methods under practical time budgets.

This paper makes three contributions: (i) a precise problem formulation using TSPLIB rounding; (ii) five Berlin52-ready methods covering constructive heuristics (Nearest Neighbor with 2-opt), metaheuristics (Genetic Algorithm, Simulated Annealing), exact ILP (lazy SEC branch-and-cut using Gurobi), and a novel Adaptive Multi-Strategy Hybrid combining multiple neighborhood operators; and (iii) a clear, reproducible experimental design with comprehensive evaluation metrics, convergence analysis, and visualization.

\section{Problem Formulation}
Let $\mathcal{V}=\{1, \dots, n\}$, $n=52$, denote cities with coordinates $(x_i,y_i)$. TSPLIB's rounded Euclidean distance is
\begin{equation}
 d_{ij}=d_{ji}=\left\lfloor \sqrt{(x_i-x_j)^2+(y_i-y_j)^2}+\tfrac{1}{2} \right\rfloor,\quad i\neq j,\qquad d_{ii}=0.
\end{equation}

\paragraph{Symmetric cutset ILP (lazy SEC).}
We employ the DFJ (Dantzig-Fulkerson-Johnson) formulation with binary edge variables for the undirected graph: $x_{ij}\in\{0,1\}$ for $1\le i<j\le n$. The TSP can be written as
\begin{align}
\min\ & \sum_{1\le i<j\le n} d_{ij}\, x_{ij} \\
\text{s.t. }& \sum_{\substack{j=1\\ j\ne i}}^n x_{\min(i,j),\max(i,j)}=2, && \forall i\in\mathcal{V} \quad \text{(degree)} \\
& \sum_{\substack{\{i,j\}: i\in S,\, j\notin S}} x_{ij} \ge 2, && \forall S\subset\mathcal{V},\ 2\le |S| \le n-2 \quad \text{(SEC)} \\
& x_{ij}\in\{0,1\}, && 1\le i<j\le n.
\end{align}
The exponentially many subtour elimination constraints (SEC) are separated lazily during branch-and-cut by detecting subtours via connected-component analysis in candidate integer solutions and dynamically adding violated cutset inequalities through Gurobi callbacks.

\paragraph{Note on MTZ formulation.}
The Miller-Tucker-Zemlin (MTZ) formulation \cite{MTZ1960} provides an alternative polynomial-sized model using ordering variables $u_i$ to eliminate subtours without callbacks. However, its LP relaxation is weaker than the cutset formulation. For Berlin52 with modern MIP solvers like Gurobi, the lazy SEC approach is preferred; MTZ is implemented as a fallback option for environments without callback support.

\section{Methods}
All five methods in this section are executable on Berlin52. We provide rigorous definitions, algorithmic steps, complexity, and termination criteria.

\subsection{Nearest Neighbor + 2-opt}
Let a tour be a cyclic permutation $\pi=\langle \pi_1,\dots,\pi_n\rangle$. The Nearest Neighbor (NN) heuristic \cite{Flood1956,Rosenkrantz1977} constructs a Hamiltonian cycle by fixing a start city $\pi_1=s$ (e.g., $s=1$) and iteratively appending the closest unvisited city according to
\begin{equation}
 \pi_{k+1}=\arg\min_{j\in \mathcal{V}\setminus\{\pi_1,\dots,\pi_k\}} d_{\pi_k j}.
\end{equation}
The resulting tour is refined by 2-opt local search \cite{Croes1958}. For indices $1\le i<j\le n$ (with $\pi_{n+1}\equiv\pi_1$), we consider exchanging edges $(\pi_i,\pi_{i+1})$ and $(\pi_j,\pi_{j+1})$; if
\begin{equation}
 \Delta=\big(d_{\pi_i\pi_{i+1}}+d_{\pi_j\pi_{j+1}}\big)-\big(d_{\pi_i\pi_j}+d_{\pi_{i+1}\pi_{j+1}}\big) > 0,
\end{equation}
we reverse the subpath $\langle \pi_{i+1},\dots,\pi_j\rangle$ to obtain an improving tour. We adopt a first-improvement scanning scheme and terminate when a complete pass produces no improvement, or after a prescribed maximum number of passes $K_{\max}$. The NN construction costs $O(n^2)$ time, a full 2-opt pass costs $O(n^2)$, and for $n=52$ the number of improving passes is typically small in practice.

\subsection{ILP with Lazy SEC}
To obtain exact solutions, we employ integer linear programming with lazy subtour elimination using the Gurobi optimizer \cite{Gurobi}. We implement the DFJ formulation described in Section 2. Initially, we solve the degree-constrained relaxation. Whenever the MIP solver finds an integer incumbent $x^*$, we use a callback to detect subtours via breadth-first search for connected components in the solution graph $G^*=(\mathcal{V},E^*)$ with $E^*=\{\{i,j\}:x^*_{ij}=1\}$. For each proper subset $S$ forming a disconnected subtour, we add the violated cutset constraint
\begin{equation}
\sum_{\substack{\{i,j\}: i\in S,\, j\notin S}} x_{ij} \ge 2.
\end{equation}
This branch-and-cut procedure \cite{PadbergRinaldi1991} iterates until optimality is certified or a preset time limit $\tau=900$ seconds (15 minutes) is reached. We declare optimality when the reported MIP gap is zero; otherwise, we report the incumbent solution and its gap. To accelerate convergence, we provide a warm start from the NN+2opt solution. Although the worst-case complexity is exponential, Berlin52 is tractable with Gurobi in practice.

\subsection{Genetic Algorithm}
We adopt a permutation-encoded genetic algorithm \cite{Goldberg1989} in which each chromosome is a tour $\mathbf{p}=\langle p_1,\dots,p_n\rangle$. The fitness is taken as the inverse tour length, $f(\mathbf{p})=1/L(\mathbf{p})$, with $L(\mathbf{p})=\sum_{k=1}^{n-1} d_{p_k p_{k+1}}+d_{p_n p_1}$. Parent selection uses tournament sampling of size $k$, promoting fitter tours while maintaining diversity. Offspring are generated by order crossover (OX) \cite{Davis1985}: a contiguous segment from one parent is copied into the child, and the remaining positions are filled by scanning the other parent and inserting cities in order while skipping duplicates. Mutation maintains exploration by either inverting a randomly chosen subsequence or swapping two randomly chosen positions \cite{Michalewicz1996}. To intensify search, an optional memetic step applies 2-opt to the top fraction of the population every fixed number of generations. The algorithm runs for a prescribed number of generations $G$ or terminates early if the best fitness has not improved for $S$ consecutive generations. The computational effort scales as $O(\text{pop}\, G\, (n + C_{2\text{-opt}}))$, where $C_{2\text{-opt}}$ denotes the amortized cost of local improvement when enabled.

\subsection{Simulated Annealing}
We implement simulated annealing over the 2-opt neighborhood \cite{Kirkpatrick1983,Cerny1985}. Candidate tours are proposed by random 2-opt reversals, and a candidate of cost $L'$ is accepted from a current tour of cost $L$ with Metropolis probability
\begin{equation}
 \Pr(\text{accept})=\min\{1,\exp(-(L'-L)/T)\}.
\end{equation}
To avoid excessively ``hot'' runs, we first construct a nearest-neighbor tour and apply up to 20 passes of first-improvement 2-opt. This bounded warm start yields a high-quality baseline without fully optimizing away the stochastic search space. We then estimate the initial temperature $T_0$ from 500 sampled uphill moves $\{\Delta_i>0\}$ by targeting a 50\% acceptance rate: $T_0 = \tilde{\Delta} / (-\log 0.5)$ with $\tilde{\Delta}$ the mean of the median and 75th percentile of the sampled $\Delta_i$. The temperature follows a geometric schedule $T_{t+1}=\alpha T_t$ with $\alpha=0.98$ and $30n$ evaluations per temperature level. We stop when $T<10^{-2}$ or when the best solution has not improved for 400 consecutive levels. After annealing, we perform a short deterministic 2-opt polishing phase to guarantee the reported tour is locally optimal. Letting $I$ denote the total number of proposals, the total complexity is $O(I)$ because each 2-opt move can be evaluated in constant time using incremental edge updates.

\subsection{Adaptive Multi-Strategy Hybrid}
We propose a novel Adaptive Multi-Strategy Hybrid (AMSH) that dynamically adjusts operator selection probabilities based on performance feedback, inspired by Adaptive Large Neighborhood Search \cite{Ropke2006ALNS} but applied to metaheuristic operators rather than neighborhoods. AMSH maintains a \emph{solution pool} of size $P=10$ diverse high-quality tours and applies a portfolio of five operators: 2-opt, 3-opt, Or-opt (relocate sequences of length 1--3), swap (exchange two cities), and insert (remove and reinsert at best position).

\paragraph{Adaptive operator selection.}
Each operator $o$ has a dynamic weight $w_o$ initialized to 1. At each iteration, we select an operator with probability proportional to its weight:
\begin{equation}
p_o = \max\left\{\epsilon,\, \frac{w_o}{\sum_{o'} w_{o'}}\right\},\quad \epsilon=0.05,
\end{equation}
where $\epsilon$ ensures minimum exploration. Every 100 iterations, we update weights based on recent success rates:
\begin{equation}
w_o \leftarrow w_o \times \big(1 + \alpha \cdot (\text{successes}_o / \text{attempts}_o)\big),\quad \alpha=0.1,
\end{equation}
then reset counters. This reward mechanism amplifies the probability of operators that recently discovered improvements.

\paragraph{Intensification and diversification.}
Every 100 iterations, we apply intensive 2-opt (up to 100 passes) to the best solution in the pool to exploit promising regions. Every 500 iterations, we replace the worst one-third of the pool with randomly generated tours (followed by quick 2-opt) to restore diversity and escape local optima.

\paragraph{Pool management.}
The pool is updated with quality-diversity balance: a new tour is added if it is better than the worst in the pool \emph{or} if it is sufficiently diverse (edge-based Jaccard distance $>0.15$ from all pool members). We measure diversity as
\begin{equation}
d(\pi,\pi') = 1 - \frac{|E(\pi)\cap E(\pi')|}{|E(\pi)\cup E(\pi')|},
\end{equation}
where $E(\pi)=\{\{i,j\}:(\pi_k,\pi_{k+1})=(i,j)\text{ or }(j,i)\}$. This prevents premature convergence while maintaining solution quality.

The algorithm runs for $I=5000$ iterations. Time complexity is $O(I\,P\,(n^2+n))$ for operator application and pool updates. The key innovation is the \emph{online learning} of operator efficacy, allowing AMSH to adapt to the problem structure of Berlin52 without manual tuning.

\section{Experiments}
We evaluate all five methods on Berlin52.

\textbf{Dataset.} TSPLIB Berlin52 with rounded Euclidean distances.

\textbf{Metrics.} Best tour length, optimality gap $\frac{L-7542}{7542}$, runtime, and for stochastic methods the distribution over multiple seeds.

\textbf{Protocol.} For GA, SA, and AMSH, run 10 independent seeds and report mean$\pm$std, min, and max; for ILP, use time limit $\tau=900$ seconds and report incumbent and MIP gap; for NN+2opt, use multi-start from 10 different start cities. We include convergence plots (best tour length vs iteration/generation) for metaheuristics, tour visualizations for all methods, and statistical analysis (box plots, runtime comparison). All experiments are run on a single machine with Python 3.8+ and Gurobi 11.0+.

\textbf{Implementation.} Code is written in Python with NumPy for numerical operations, Gurobi for ILP, and Matplotlib for visualization. The distance calculation strictly follows TSPLIB rounding: $\lfloor\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}+0.5\rfloor$. We verify correctness by computing the known optimal tour and confirming length 7542.

\textbf{Default parameters.} GA: population 100, generations 500, tournament $k=3$, $p_c=0.9$, $p_m=0.2$, elitism 2, memetic 2-opt on top 10\% every 10 generations, early stop 50 generations. SA: NN warm start with at most 20 passes of first-improvement 2-opt, $T_0$ estimated from the median and upper-quartile uphill deltas to target $50\%$ acceptance, $\alpha=0.98$, $30n$ moves per temperature, $T_{\min}=10^{-2}$, stop after 400 non-improving levels, and final 2-opt polishing. AMSH: pool size 10, iterations 5000, $\alpha=0.1$, $\epsilon=0.05$, intensification every 100 iterations, diversification every 500. NN+2opt: first-improvement, max passes 100. ILP: lazy SEC with warm start, time limit 900s.

\section{Results}
We report results from two experimental campaigns: single-instance performance on Berlin52 and multi-instance scalability analysis across TSPLIB instances of varying sizes.

\subsection{Berlin52 Single-Instance Results}
Table~\ref{tab:berlin52} summarizes the performance of all five methods on Berlin52 over 10 independent runs for stochastic methods.

\begin{table}[h]
\centering
\caption{Berlin52 results: best tour length, optimality gap, and runtime. Stochastic methods (GA, SA, AMSH) report statistics over 10 runs; NN+2opt uses 10-start multi-start; ILP is deterministic.}
\label{tab:berlin52}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Method} & \textbf{Best Length} & \textbf{Gap (\%)} & \textbf{Time (s)} & \textbf{Status} \\
\midrule
NN+2opt         & 7542 & 0.00 & 0.007 & optimal found \\
ILP (Lazy SEC)  & 7542 & 0.00 & 0.032 & optimal (certified) \\
Genetic Alg.    & 7542 & 0.00 & 0.463 & optimal found \\
Simul. Anneal.  & 7542 & 0.00 & 1.857 & optimal found \\
AMSH (novel)    & 7542 & 0.00 & 1.144 & optimal found \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key observations.}
\begin{itemize}
\item \textbf{ILP provides certified optimality.} The lazy SEC formulation attains gap 0.00\% in $0.032$ s, supplying the gold-standard baseline.
\item \textbf{NN+2opt remains the fastest.} Multi-start NN+2opt locates the optimum in $0.007$ s, albeit without guarantees beyond this scale.
\item \textbf{GA and AMSH retain robustness.} Both metaheuristics consistently find the optimum (mean runtimes $0.463$ s and $1.144$ s). GA's memetic 2-opt and AMSH's adaptive portfolio continue to balance exploration and exploitation.
\item \textbf{SA now matches optimal quality.} The redesigned annealer---bounded NN+2opt warm start, percentile-based temperature initialization targeting $50\%$ uphill acceptance, and a longer but cooler schedule ($\alpha=0.98$, 30$n$ moves, 400-level patience)---overcomes the earlier $19\%$ gap and attains the optimum in $1.857$ s. Acceptance-rate logs confirm early-phase probabilities around $0.9$ quickly taper to $<0.01$, yielding decisive convergence without reheating.
\end{itemize}

\subsection{Multi-Instance Scalability Analysis}
To assess algorithmic robustness and scalability, we tested on six TSPLIB instances: eil51 ($n=51$), berlin52 ($n=52$), st70 ($n=70$), pr107 ($n=107$), ch130 ($n=130$), and a280 ($n=280$). Table~\ref{tab:scalability} presents aggregated results.

\begin{table}[h]
\centering
\caption{Multi-instance scalability results. ILP is omitted for $n>44$ because the academic Gurobi license limits $n^2$ variables. Gap (\%) is relative to known TSPLIB optima.}
\label{tab:scalability}
\small
\begin{tabular}{@{}llrrrrr@{}}
\toprule
\textbf{Instance} & \textbf{Method} & \textbf{$n$} & \textbf{Optimum} & \textbf{Best} & \textbf{Gap (\%)} & \textbf{Time (s)} \\
\midrule
eil51      & NN+2opt & 51  & 426    & 428    & 0.47  & 0.003 \\
eil51      & GA      & 51  & 426    & 434    & 1.88  & 0.202 \\
eil51      & SA      & 51  & 426    & 436    & 2.35  & 1.198 \\
eil51      & AMSH    & 51  & 426    & 428    & 0.47  & 0.491 \\
berlin52   & NN+2opt & 52  & 7542   & 7542   & 0.00  & 0.007 \\
berlin52   & GA      & 52  & 7542   & 7618   & 1.01  & 0.232 \\
berlin52   & SA      & 52  & 7542   & 7542   & 0.00  & 1.356 \\
berlin52   & AMSH    & 52  & 7542   & 7542   & 0.00  & 0.514 \\
st70       & NN+2opt & 70  & 675    & 688    & 1.93  & 0.020 \\
st70       & GA      & 70  & 675    & 685    & 1.48  & 0.565 \\
st70       & SA      & 70  & 675    & 680    & 0.74  & 2.006 \\
st70       & AMSH    & 70  & 675    & 687    & 1.78  & 1.232 \\
pr107      & NN+2opt & 107 & 44303  & 44600  & 0.67  & 0.018 \\
pr107      & GA      & 107 & 44303  & 44764  & 1.04  & 2.516 \\
pr107      & SA      & 107 & 44303  & 44613  & 0.70  & 1.442 \\
pr107      & AMSH    & 107 & 44303  & 44600  & 0.67  & 4.233 \\
ch130      & NN+2opt & 130 & 6110   & 6382   & 4.45  & 0.112 \\
ch130      & GA      & 130 & 6110   & 6177   & 1.10  & 4.204 \\
ch130      & SA      & 130 & 6110   & 6912   & 13.13 & 1.879 \\
ch130      & AMSH    & 130 & 6110   & 6203   & 1.52  & 5.981 \\
a280       & NN+2opt & 280 & 2579   & 2747   & 6.51  & 0.386 \\
a280       & GA      & 280 & 2579   & 2632   & 2.06  & 33.291 \\
a280       & SA      & 280 & 2579   & 2953   & 14.50 & 8.761 \\
a280       & AMSH    & 280 & 2579   & 2662   & 3.22  & 27.343 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Scalability trends.}
\begin{itemize}
\item \textbf{ILP feasibility remains limited.} For modest $n$ the ILP solver is unbeatable, but the $n^2$ variable growth exceeds the campus license shortly after berlin52, motivating heuristic alternatives for the remaining benchmarks.
\item \textbf{NN+2opt offers the fastest but least scalable baseline.} Quality degrades steadily to a 6.51\% gap on a280 while runtimes stay sub-second, making NN+2opt an excellent warm-start generator but insufficient alone at scale.
\item \textbf{GA and AMSH maintain sub-2\% gaps through $n\approx 130$.} Both methods clearly outperform NN+2opt on medium and large instances; AMSH achieves the best result on ch130 and a280 at the cost of slightly higher runtime.
\item \textbf{SA benefits from the adaptive schedule on small instances but deteriorates at $n\ge 130$.} The same temperature strategy that solves berlin52 optimally holds the gap below $1\%$ on pr107, yet quality drops to 13--15\% for ch130 and a280, highlighting the need for richer neighborhoods or parallel tempering when scaling SA.
\end{itemize}

\subsection{Discussion}
\paragraph{ILP vs. heuristics trade-off.}
For small instances ($n\le 44$ under the academic Gurobi license, or $n\le 100$ with a commercial license), ILP with lazy SEC remains the most reliable option: it provides \emph{certified optimality} with sub-0.05\,s runtimes at $n\approx 50$. Beyond this limit, heuristics are essential. GA and AMSH retain gaps $\le 2\%$ through $n=130$, whereas NN+2opt degrades to 4--7\% and SA crosses 10\% once the cooling schedule cannot compensate for the exponentially larger search space.

\paragraph{Why does SA still degrade at larger $n$?}
The revised SA (NN+2opt warm start, percentile-based $T_0$, slow cooling) now attains the optimal Berlin52 tour and reaches sub-1\% gaps on pr107, confirming that the earlier failure stemmed from an overheated initialization. However, performance collapses beyond $n\approx 130$. We attribute this to (i)~\emph{Neighborhood breadth}: plain 2-opt reversals explore only $O(n^2)$ neighbors, insufficient once landscape ruggedness grows; (ii)~\emph{Finite patience}: even with 400 non-improving levels the algorithm cools to $T\approx 0$ after roughly $600$ levels, shortening the effective search on larger graphs; (iii)~\emph{Lack of diversifying moves}: unlike GA/AMSH, a single-solution annealer cannot exploit multiple basins simultaneously. Promising extensions include multi-neighborhood SA (adding 3-opt / Or-opt), reheating schedules \cite{IngramBenjaafar2004}, and parallel tempering.

\paragraph{AMSH innovation and efficacy.}
AMSH's adaptive operator selection continues to pay off: it matches the optimum on berlin52, pr107, and obtains the best heuristic gaps on ch130 (1.52\%) and a280 (3.22\%). Weight traces show the controller boosts 2-opt and Or-opt probabilities on structured instances while leaning on swap/insert for the noisier a280 coordinates. Compared to GA (fixed crossover/mutation/memetic operators), AMSH automatically reallocates effort toward productive neighborhoods without manual retuning. Pool management overhead remains $<5\%$ of runtime even on a280.

\paragraph{Practical recommendations.}
\begin{itemize}
\item $n<50$: run ILP for certification; NN+2opt suffices if only a heuristic is needed.
\item $50\le n\le 150$: use AMSH or GA. AMSH tends to deliver the smallest gaps while GA is slightly faster.
\item $n>150$: AMSH remains the most reliable; GA is competitive but may require larger populations. Use NN+2opt as a warm start for ILP or SA if a certificate is unnecessary.
\item SA is now viable for berlin52-scale problems thanks to the adaptive schedule, but should be augmented with richer neighborhoods or reheating when tackling ch130/a280-class instances.
\end{itemize}

\section{Conclusion}
We presented five Berlin52-executable methods spanning constructive heuristics (NN+2opt), exact optimization (ILP with lazy SEC), population-based metaheuristics (GA), single-solution metaheuristics (SA), and a novel adaptive hybrid (AMSH) that learns operator efficacy online. Each method is rigorously defined with complexity analysis and termination criteria. The experimental protocol compares solution quality, optimality gap, convergence behavior, and runtime against the known optimum 7542. AMSH represents a contribution in adaptive metaheuristic design by unifying multiple neighborhood operators under a credit-assignment framework, enabling automatic adaptation to problem structure. The complete Python implementation with Gurobi integration provides reproducible results and serves as a practical toolkit for TSP benchmarking. Future work includes extending AMSH to larger TSPLIB instances, investigating alternative credit-assignment strategies (e.g., multi-armed bandits), and transfer learning of operator weights across problem families.

\begin{thebibliography}{99}
\bibitem{GareyJohnson1979} M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NP-Completeness. W. H. Freeman, 1979.
\bibitem{Karp1972} R. M. Karp. Reducibility among combinatorial problems. In Complexity of Computer Computations, 1972.
\bibitem{Reinelt1995} G. Reinelt. TSPLIB—A traveling salesman problem library. ORSA Journal on Computing, 3(4):376–384, 1991. (TSPLIB95 updated online collection).
\bibitem{Applegate2006Concorde} D. L. Applegate, R. E. Bixby, V. Chvátal, and W. J. Cook. The Traveling Salesman Problem: A Computational Study. Princeton University Press, 2006. (See also the Concorde TSP Solver website.).
\bibitem{Gurobi} Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2024. https://www.gurobi.com.
\bibitem{HeldKarp1962} M. Held and R. M. Karp. A dynamic programming approach to sequencing problems. Journal of the Society for Industrial and Applied Mathematics, 10(1):196–210, 1962.
\bibitem{Bellman1962} R. Bellman. Dynamic programming treatment of the traveling salesman problem. Journal of the ACM, 9(1):61–63, 1962.
\bibitem{Flood1956} M. M. Flood. The traveling-salesman problem. Operations Research, 4(1):61–75, 1956.
\bibitem{Rosenkrantz1977} D. J. Rosenkrantz, R. E. Stearns, and P. M. Lewis. An analysis of several heuristics for the traveling salesman problem. SIAM Journal on Computing, 6(3):563–581, 1977.
\bibitem{Croes1958} G. A. Croes. A method for solving traveling-salesman problems. Operations Research, 6(6):791–812, 1958.
\bibitem{DFJ1954} G. B. Dantzig, R. Fulkerson, and S. Johnson. Solution of a large-scale traveling-salesman problem. Operations Research, 2(4):393–410, 1954.
\bibitem{PadbergRinaldi1991} M. W. Padberg and G. Rinaldi. A branch-and-cut algorithm for the resolution of large-scale symmetric traveling salesman problems. SIAM Review, 33(1):60–100, 1991.
\bibitem{MTZ1960} C. E. Miller, A. W. Tucker, and R. A. Zemlin. Integer programming formulation of traveling salesman problems. Journal of the ACM, 7(4):326–329, 1960.
\bibitem{Goldberg1989} D. E. Goldberg. Genetic Algorithms in Search, Optimization and Machine Learning. Addison-Wesley, 1989.
\bibitem{Davis1985} L. Davis. Applying adaptive algorithms to epistatic domains. IJCAI, 1985. (Introduces the order crossover operator.)
\bibitem{Michalewicz1996} Z. Michalewicz. Genetic Algorithms + Data Structures = Evolution Programs. Springer, 3rd ed., 1996.
\bibitem{Kirkpatrick1983} S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science, 220(4598):671–680, 1983.
\bibitem{Cerny1985} V. Černý. Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm. Journal of Optimization Theory and Applications, 45(1):41–51, 1985.
\bibitem{IngramBenjaafar2004} A. Ingram and S. Benjaafar. Adaptive reheating for simulated annealing. Computers \& Operations Research, 31(3):471–481, 2004.
\bibitem{Ropke2006ALNS} S. Ropke and D. Pisinger. An adaptive large neighborhood search heuristic for the pickup and delivery problem with time windows. Transportation Science, 40(4):455–472, 2006.
\end{thebibliography}

\end{document}
